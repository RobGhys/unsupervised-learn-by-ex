{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes for Homework 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a) fitting histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p_θ(x) = exp(θx) / Σ exp(θx')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a softmax model. Let's see how to compute it with a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define theta as an array of shape 'd'\n",
    "# let's say d=3\n",
    "theta: np.ndarray = np.array([0.5, 1.0, -0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33149896, 0.54654939, 0.12195165])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We compute all the probabilities at once with numpy\n",
    "# it is a softargmax...\n",
    "probs: np.ndarray = np.exp(theta) / np.sum(np.exp(theta))\n",
    "assert np.sum(probs) == 1.0\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Log Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L(θ) = - (1/N) Σ log(p_θ(x_i))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in the data, there are : one 0, two 1, one 2\n",
    "# 3 datapoints, matching len(probs)\n",
    "data = np.array([0, 2, 1, 1])\n",
    "assert len(np.unique(data)) == len(probs)\n",
    "n = len(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.1041306053367284)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_theta = (-1/n) * np.sum(np.log(probs[data]))\n",
    "loss_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use np to directly compute 1/n * sum ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.1041306053367284)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_theta_better = -np.mean(np.log(probs[data]))\n",
    "loss_theta_better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is probs[data] ?\n",
    "* probs is an array containing the probability for each class\n",
    "* data is the observations of each class\n",
    "* probs[data] is getting in the 'probs' array, the probability assigned to each observation. So this is 'as the model believes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33149896, 0.12195165, 0.54654939, 0.54654939])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert loss_theta == loss_theta_better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving the Gradient\n",
    "$∇L = - (1/N) Σ (e_xi - p_θ)$\n",
    "\n",
    "The gradient is given above. Where does it come from?\n",
    "\n",
    "**1. Starting with Negative Log Likelihood**\n",
    "\n",
    "The loss function is:\n",
    "$$L(\\theta) = -\\frac{1}{N} \\sum \\log(p_\\theta(x_i))$$\n",
    "where: $$p_\\theta(x) = \\frac{\\exp(\\theta_x)}{\\sum_j \\exp(\\theta_j)}$$\n",
    "\n",
    "**2. Taking the Derivative**\n",
    "\n",
    "For one example $x$, let's derive $\\log(p_\\theta(x))$ with respect to $\\theta_k$:\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial}{\\partial\\theta_k} \\log(p_\\theta(x)) &= \\frac{\\partial}{\\partial\\theta_k} [\\log(\\exp(\\theta_x)) - \\log(\\sum_j \\exp(\\theta_j))] \\\n",
    "&= \\frac{\\partial}{\\partial\\theta_k} [\\theta_x - \\log(\\sum_j \\exp(\\theta_j))]\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "**3. Two Cases to Consider.**\n",
    "\n",
    "Either k=x (case 1), which means that $\\theta(k) == \\theta(x)$, then $\\frac{\\partial}{\\partial\\theta_k} = 1$.\n",
    "\n",
    "Or $k \\neq x$, then $\\frac{\\partial}{\\partial\\theta_k} = 0$.\n",
    "\n",
    "Case 1: $k = x$ (observed class)\n",
    "$$\\frac{\\partial}{\\partial\\theta_k} [\\theta_x - \\log(\\sum_j \\exp(\\theta_j))] = 1 - \\frac{\\exp(\\theta_k)}{\\sum_j \\exp(\\theta_j)} = 1 - p_\\theta(k)$$\n",
    "\n",
    "Case 2: $k \\neq x$\n",
    "$$\\frac{\\partial}{\\partial\\theta_k} [\\theta_x - \\log(\\sum_j \\exp(\\theta_j))] = 0 - \\frac{\\exp(\\theta_k)}{\\sum_j \\exp(\\theta_j)} = -p_\\theta(k)$$\n",
    "\n",
    "**4. Combining Results**\n",
    "\n",
    "This gives us:\n",
    "$$\\frac{\\partial L}{\\partial\\theta_k} = -\\frac{1}{N} \\sum [1{x=k} - p_\\theta(k)]$$\n",
    "where $1{x=k}$ is the indicator function: 1 if $x=k$, 0 otherwise.\n",
    "\n",
    "**5. Final Form (vectorized)**\n",
    "$$\\nabla L = -\\frac{1}{N} \\sum (e_x - p_\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b) Fitting Discretized Mixture of Logistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to initialize the coefficients of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 20\n",
    "n_logistics_models: int = 4\n",
    "theta = np.zeros((n_logistics_models, 3))\n",
    "theta[:, 0] = np.random.uniform(0, d-1, size=n_logistics_models) # mean initialized at random, with values between [0, d-1]\n",
    "theta[:, 1] = np.exp(np.random.rand(n_logistics_models)) # s\n",
    "theta[:, 2] = np.full(n_logistics_models, (1 / n_logistics_models)) # pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use them efficiently, in vectorized form, we need to use Numpy's broadcasting feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.61667106,  1.50266247, 12.50647138, 13.26991431])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape (4)\n",
    "theta[:, 0] # mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [None] is equivalent to unsqueeze() in PyTorch.\n",
    "mu = theta[:, 0][:, None] # shape (4, 1)\n",
    "s = theta[:, 1][:, None] # shape (4, 1)\n",
    "pi = theta[:, 2][:, None] # shape (4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.61667106],\n",
       "       [ 1.50266247],\n",
       "       [12.50647138],\n",
       "       [13.26991431]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu # shepe (4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dérivation du Gradient pour un Mélange de Logistiques Discrétisées\n",
    "\n",
    "## Rappel du modèle\n",
    "Notre modèle de probabilité est:\n",
    "$p_\\theta(x) = \\sum_{i=1}^4 \\pi_i[\\sigma(\\frac{x+0.5 - \\mu_i}{s_i}) - \\sigma(\\frac{x-0.5-\\mu_i}{s_i})]$\n",
    "\n",
    "où $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ est la fonction sigmoïde.\n",
    "\n",
    "## Loss Function\n",
    "La negative log-likelihood est:\n",
    "$L = -\\frac{1}{N}\\sum_{n=1}^N \\log(p_\\theta(x_n))$\n",
    "\n",
    "## Dérivées partielles\n",
    "\n",
    "### Par rapport à μᵢ\n",
    "$\\frac{\\partial L}{\\partial \\mu_i} = -\\frac{1}{N}\\sum_{n=1}^N \\frac{1}{p_\\theta(x_n)} \\cdot \\pi_i[\\frac{\\partial \\sigma}{\\partial \\mu_i}(\\frac{x_n+0.5 - \\mu_i}{s_i}) - \\frac{\\partial \\sigma}{\\partial \\mu_i}(\\frac{x_n-0.5-\\mu_i}{s_i})]$\n",
    "\n",
    "où $\\frac{\\partial \\sigma}{\\partial \\mu_i}(z) = \\sigma(z)(1-\\sigma(z))(-\\frac{1}{s_i})$\n",
    "\n",
    "### Par rapport à sᵢ\n",
    "$\\frac{\\partial L}{\\partial s_i} = -\\frac{1}{N}\\sum_{n=1}^N \\frac{1}{p_\\theta(x_n)} \\cdot \\pi_i[\\frac{\\partial \\sigma}{\\partial s_i}(\\frac{x_n+0.5 - \\mu_i}{s_i}) - \\frac{\\partial \\sigma}{\\partial s_i}(\\frac{x_n-0.5-\\mu_i}{s_i})]$\n",
    "\n",
    "où $\\frac{\\partial \\sigma}{\\partial s_i}(z) = \\sigma(z)(1-\\sigma(z))(-\\frac{z}{s_i})$\n",
    "\n",
    "### Par rapport à πᵢ\n",
    "$\\frac{\\partial L}{\\partial \\pi_i} = -\\frac{1}{N}\\sum_{n=1}^N \\frac{1}{p_\\theta(x_n)} \\cdot [\\sigma(\\frac{x_n+0.5 - \\mu_i}{s_i}) - \\sigma(\\frac{x_n-0.5-\\mu_i}{s_i})]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Forme générale du gradient\n",
    "\n",
    "Pour un point $x_n$, définissons:\n",
    "$\\alpha_i(x_n) = \\frac{x_n+0.5 - \\mu_i}{s_i}$ et $\\beta_i(x_n) = \\frac{x_n-0.5 - \\mu_i}{s_i}$\n",
    "\n",
    "Alors la contribution de chaque point $x_n$ au gradient peut s'écrire:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\theta_i} = -\\frac{1}{N}\\sum_{n=1}^N \\frac{1}{p_\\theta(x_n)} \\cdot \\pi_i \\cdot [\\sigma'(\\alpha_i(x_n)) \\cdot \\frac{\\partial \\alpha_i}{\\partial \\theta_i} - \\sigma'(\\beta_i(x_n)) \\cdot \\frac{\\partial \\beta_i}{\\partial \\theta_i}]$\n",
    "\n",
    "où:\n",
    "- Pour $\\mu_i$: $\\frac{\\partial \\alpha_i}{\\partial \\mu_i} = \\frac{\\partial \\beta_i}{\\partial \\mu_i} = -\\frac{1}{s_i}$\n",
    "- Pour $s_i$: $\\frac{\\partial \\alpha_i}{\\partial s_i} = -\\frac{\\alpha_i}{s_i}$ et $\\frac{\\partial \\beta_i}{\\partial s_i} = -\\frac{\\beta_i}{s_i}$\n",
    "- Pour $\\pi_i$: c'est un cas spécial où le $\\pi_i$ sort et les dérivées partielles sont 1\n",
    "\n",
    "Et $\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$ est la dérivée de la fonction sigmoïde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
