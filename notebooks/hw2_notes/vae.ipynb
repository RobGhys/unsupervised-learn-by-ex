{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evidence Lower BOund (ELBO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to max the likelihood p(x) of the data. But p(x) is intractable. We need a lower bound, i.e., ELBO that we can optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder (VAE) - Formulation mathématique\n",
    "\n",
    "## Distribution Prior\n",
    "Le prior sur la variable latente $z$ est une distribution normale standard:\n",
    "$p(z) = \\mathcal{N}(0, I)$\n",
    "\n",
    "## Encoder (Approximate Posterior)\n",
    "L'encodeur produit une distribution gaussienne avec moyenne et variance pour chaque entrée $x$:\n",
    "$q_\\theta(z|x) = \\mathcal{N}(z; \\mu_\\theta(x), \\Sigma_\\theta(x))$\n",
    "\n",
    "où:\n",
    "- $\\mu_\\theta(x)$ est le vecteur moyenne (2D)\n",
    "- $\\Sigma_\\theta(x)$ est la matrice de covariance diagonale (2D)\n",
    "\n",
    "## Decoder\n",
    "Le décodeur produit également une distribution gaussienne:\n",
    "$p(x|z) = \\mathcal{N}(x; \\mu_\\phi(z), \\Sigma_\\phi(z))$\n",
    "\n",
    "où:\n",
    "- $\\mu_\\phi(z)$ est le vecteur moyenne (dimension d'entrée)\n",
    "- $\\Sigma_\\phi(z)$ est la matrice de covariance diagonale (dimension d'entrée)\n",
    "\n",
    "## Reparametrization Trick\n",
    "Pour permettre la rétropropagation à travers l'échantillonnage:\n",
    "$z = \\mu_\\theta(x) + \\Sigma_\\theta(x)^{1/2} \\odot \\epsilon, \\text{ où } \\epsilon \\sim \\mathcal{N}(0, I)$\n",
    "\n",
    "## Evidence Lower BOund (ELBO)\n",
    "L'objectif est de maximiser:\n",
    "$\\text{ELBO} = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - \\text{KL}(q_\\theta(z|x) \\| p(z))$\n",
    "\n",
    "Cette expression se décompose en:\n",
    "\n",
    "1. **Terme de reconstruction**:\n",
    "$\\mathbb{E}_{q(z|x)}[\\log p(x|z)]$\n",
    "\n",
    "Pour une distribution gaussienne:\n",
    "$-\\log p(x|z) = \\frac{1}{2} (\\log(2\\pi) + \\log(\\Sigma_\\phi(z)) + \\frac{(x - \\mu_\\phi(z))^2}{\\Sigma_\\phi(z)})$\n",
    "\n",
    "2. **Terme KL**:\n",
    "\n",
    "Pour des distributions gaussiennes, il a une forme fermée:\n",
    "$\\text{KL} = \\frac{1}{2} \\sum (\\mu_\\theta(x)^2 + \\Sigma_\\theta(x) - \\log(\\Sigma_\\theta(x)) - 1)$\n",
    "\n",
    "## Loss totale\n",
    "On minimise la negative ELBO:\n",
    "$\\mathcal{L} = -\\mathbb{E}_{q(z|x)}[\\log p(x|z)] + \\text{KL}(q_\\theta(z|x) \\| p(z))$\n",
    "\n",
    "## Dimensions\n",
    "- $x \\in \\mathbb{R}^D$ (dimension d'entrée)\n",
    "- $z \\in \\mathbb{R}^2$ (dimension latente = 2)\n",
    "- $\\mu_\\theta(x) \\in \\mathbb{R}^2$\n",
    "- $\\Sigma_\\theta(x) \\in \\mathbb{R}^2$ (covariance diagonale)\n",
    "- $\\mu_\\phi(z) \\in \\mathbb{R}^D$\n",
    "- $\\Sigma_\\phi(z) \\in \\mathbb{R}^D$ (covariance diagonale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction Loss pour une distribution Gaussienne\n",
    "\n",
    "## Formule générale\n",
    "Pour une distribution gaussienne, la log-vraisemblance négative (reconstruction loss) est:\n",
    "\n",
    "$-\\log p(x|z) = \\frac{1}{2} (\\log(2\\pi) + \\log(\\Sigma_\\phi(z)) + \\frac{(x - \\mu_\\phi(z))^2}{\\Sigma_\\phi(z)})$\n",
    "\n",
    "## Décomposition du terme\n",
    "1. **Terme constant**: $\\frac{1}{2}\\log(2\\pi)$\n",
    "  - Ce terme est constant et n'affecte pas l'optimisation\n",
    "  - Il peut être ignoré pendant l'entraînement\n",
    "\n",
    "2. **Terme de variance**: $\\frac{1}{2}\\log(\\Sigma_\\phi(z))$\n",
    "  - Pénalise les grandes variances\n",
    "  - Encourage le modèle à être confiant dans ses prédictions\n",
    "  - Pour une variance $\\sigma^2$ proche de 0, $\\log(\\sigma^2)$ tend vers $-\\infty$\n",
    "  - Pour une variance $\\sigma^2$ grande, $\\log(\\sigma^2)$ devient positif\n",
    "\n",
    "3. **Terme d'erreur quadratique normalisée**: $\\frac{1}{2}\\frac{(x - \\mu_\\phi(z))^2}{\\Sigma_\\phi(z)}$\n",
    "  - Mesure l'erreur de reconstruction\n",
    "  - Pondérée par l'inverse de la variance\n",
    "  - Si $\\Sigma_\\phi(z)$ est petit:\n",
    "    - Grande pénalité pour les erreurs\n",
    "    - Le modèle doit être très précis\n",
    "  - Si $\\Sigma_\\phi(z)$ est grand:\n",
    "    - Faible pénalité pour les erreurs\n",
    "    - Le modèle peut être moins précis\n",
    "\n",
    "## En pratique\n",
    "- Pour chaque dimension $i$:\n",
    "$-\\log p(x_i|z) = \\frac{1}{2} (\\log(2\\pi) + \\log(\\sigma^2_i) + \\frac{(x_i - \\mu_i)^2}{\\sigma^2_i})$\n",
    "\n",
    "- La loss totale est la somme sur toutes les dimensions:\n",
    "$-\\log p(x|z) = \\sum_i -\\log p(x_i|z)$\n",
    "\n",
    "## Trade-off Variance-Précision\n",
    "Il y a un équilibre à trouver:\n",
    "- Une variance trop faible force une reconstruction très précise mais risque d'overfitting\n",
    "- Une variance trop élevée permet trop d'imprécision\n",
    "- Le modèle doit apprendre à moduler la variance selon la difficulté de reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dérivation de la Reconstruction Loss Gaussienne\n",
    "\n",
    "## 1. Rappel densité Gaussienne\n",
    "Pour une gaussienne univariée:\n",
    "$p(x|z) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})$\n",
    "\n",
    "où $\\mu = \\mu_\\phi(z)$ et $\\sigma^2 = \\Sigma_\\phi(z)$\n",
    "\n",
    "## 2. Application du log négatif\n",
    "$-\\log p(x|z) = -\\log(\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp(-\\frac{(x-\\mu)^2}{2\\sigma^2}))$\n",
    "\n",
    "## 3. Propriétés des logs\n",
    "$-\\log p(x|z) = -[-\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(x-\\mu)^2}{2\\sigma^2}]$\n",
    "\n",
    "## 4. Distributivité du négatif\n",
    "$-\\log p(x|z) = \\frac{1}{2}\\log(2\\pi\\sigma^2) + \\frac{(x-\\mu)^2}{2\\sigma^2}$\n",
    "\n",
    "## 5. Propriété du log\n",
    "$\\log(2\\pi\\sigma^2) = \\log(2\\pi) + \\log(\\sigma^2)$\n",
    "\n",
    "## 6. Formule finale\n",
    "$-\\log p(x|z) = \\frac{1}{2}(\\log(2\\pi) + \\log(\\sigma^2) + \\frac{(x-\\mu)^2}{\\sigma^2})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
